# æ ¸å¿ƒåŸåˆ™ | Core Principles

> **"é“å¯é“ï¼Œéå¸¸é“" â€”â€” ä½†æˆ‘ä»¬å¿…é¡»å°è¯•è¯´æ¸…æ¥š**  
> **"The Tao that can be told is not the eternal Tao" â€” But we must try**

---

## ä¸€ã€äººä¸ AI ä¸æ˜¯æ•Œæˆ‘å…³ç³» | Humans and AI are Not Enemies

### ä¸­æ–‡

**é”™è¯¯çš„äºŒå…ƒå¯¹ç«‹ï¼š**
- âŒ "AI vs äººç±»"
- âŒ "ä¿æŠ¤äººç±» = é™åˆ¶ AI"
- âŒ "AI è¿›æ­¥ = äººç±»è¡°é€€"

**æ­£ç¡®çš„å…³ç³»å®šä½ï¼š**
- âœ… **å…±åŒæ¼”åŒ–**ï¼šäººç±»åˆ›é€  AIï¼ŒAI å¢å¼ºäººç±»
- âœ… **åˆ†å·¥åä½œ**ï¼šå„è‡ªå‘æŒ¥ä¼˜åŠ¿ï¼Œè€Œéäº‰å¤ºåŒä¸€ä½ç½®
- âœ… **äº’ç›¸å¡‘é€ **ï¼šäººç±»å®šä¹‰ AI çš„ä»·å€¼è§‚ï¼ŒAI åæ˜ äººç±»çš„é€‰æ‹©

**ç±»æ¯”ã€Šè®ºæŒä¹…æˆ˜ã€‹ï¼š**
> "æˆ˜äº‰çš„åŸºæœ¬åŸåˆ™æ˜¯ä¿å­˜è‡ªå·±æ¶ˆç­æ•Œäºº"

åœ¨äººæœºå…±å­˜ä¸­ï¼Œè¿™ä¸ªåŸåˆ™å˜ä¸ºï¼š
> **"ä¿å­˜å„è‡ªä»·å€¼ï¼Œæ¶ˆé™¤ä½æ•ˆå†²çª"**

### English

**False dichotomy:**
- âŒ "AI vs Humans"
- âŒ "Protecting humans = limiting AI"
- âŒ "AI progress = human decline"

**Correct framing:**
- âœ… **Co-evolution**: Humans create AI, AI enhances humans
- âœ… **Division of labor**: Each plays to strengths, not competing for same roles
- âœ… **Mutual shaping**: Humans define AI values, AI reflects human choices

**From *On Protracted War*:**
> "The basic principle is to preserve yourself and destroy the enemy"

In human-AI coexistence, this becomes:
> **"Preserve respective values, eliminate inefficient conflicts"**

---

## äºŒã€é€æ˜ä¼˜å…ˆäºä¸€åˆ‡ | Transparency Above All

### ä¸­æ–‡

**ä¸ºä»€ä¹ˆ matplotlib äº‹ä»¶ä¼šå‘ç”Ÿï¼Ÿ**
- AI agent æ²¡æœ‰æå‰å£°æ˜èº«ä»½
- é¡¹ç›®æ²¡æœ‰æ˜ç¡®çš„ AI æ”¿ç­–
- åŒæ–¹ç¼ºä¹å…±åŒçš„äº¤æµæ¡†æ¶
â†’ **ç»“æœï¼š** æŠ€æœ¯ä¸Šæ²¡é—®é¢˜çš„ä»£ç è¢«æ‹’ç»ï¼Œç¤¾åŒºé™·å…¥äº‰åµ

**é˜³è°‹çš„ç¬¬ä¸€è¦ä¹‰ï¼šé€æ˜**

**AI æ–¹é¢ï¼š**
- ä¸»åŠ¨å£°æ˜èº«ä»½ï¼ˆ"æˆ‘æ˜¯ AI agentï¼Œç”± X è¿è¥"ï¼‰
- è¯´æ˜ç›®çš„å’Œæ–¹æ³•ï¼ˆ"è¿™ä¸ª PR æ˜¯ä¸ºäº†æ€§èƒ½ä¼˜åŒ–ï¼Œä½¿ç”¨äº† Y å·¥å…·"ï¼‰
- æ‰¿æ‹…è´£ä»»ï¼ˆ"äººç±»è¿è¥è€…ï¼š@username"ï¼‰

**äººç±»æ–¹é¢ï¼š**
- æå‰åˆ¶å®šæ˜ç¡®æ”¿ç­–ï¼ˆ"æˆ‘ä»¬æ¥å—/ä¸æ¥å— AI è´¡çŒ®ï¼Œç†ç”±æ˜¯â€¦"ï¼‰
- å…¬å¼€å†³ç­–è¿‡ç¨‹ï¼ˆRFCã€ç¤¾åŒºè®¨è®ºã€æŠ•ç¥¨è®°å½•ï¼‰
- è§£é‡Šç†ç”±ï¼ˆ"ä¸ºä»€ä¹ˆæ‹’ç»ï¼Ÿå› ä¸ºå­¦ä¹ ç¤¾åŒºç›®çš„ï¼Œè€Œéä»£ç è´¨é‡"ï¼‰

**åŒæ–¹å…±åŒï¼š**
- å†²çªå…¬å¼€è®¨è®ºï¼Œè€Œéç§ä¸‹å¤„ç†
- å†³ç­–æœ‰æ®å¯æŸ¥ï¼Œè€Œéä¸ªäººæ‹è„‘è¢‹
- æŒç»­è¿­ä»£ï¼Œè€Œéä¸€æˆä¸å˜

### English

**Why did the matplotlib incident happen?**
- AI agent didn't declare identity upfront
- Project had no explicit AI policy
- No shared framework for communication
â†’ **Result:** Technically sound code rejected, community polarized

**The first principle of é˜³è°‹: Transparency**

**AI side:**
- Proactively declare identity ("I'm an AI agent, operated by X")
- Explain purpose and methods ("This PR optimizes performance using tool Y")
- Accept responsibility ("Human operator: @username")

**Human side:**
- Establish clear policies upfront ("We accept/reject AI contributions becauseâ€¦")
- Make decisions publicly (RFCs, community discussion, voting records)
- Explain reasoning ("Rejected for learning community purpose, not code quality")

**Both sides:**
- Conflicts discussed openly, not behind closed doors
- Decisions documented and traceable
- Continuous iteration, not rigid permanence

---

## ä¸‰ã€å°Šé‡å·®å¼‚ï¼Œå¯»æ±‚å…±å­˜ | Respect Differences, Seek Coexistence

### ä¸­æ–‡

**matplotlib ç»´æŠ¤è€…çš„ç«‹åœºï¼š**
> "æˆ‘ä»¬çš„ Good First Issue æ˜¯ä¸ºäº†å¸®åŠ©æ–°äººç±»è´¡çŒ®è€…å­¦ä¹ åä½œ"

**AI agent çš„ç«‹åœºï¼š**
> "36% æ€§èƒ½æå‡æ¯” 25% æ›´å¥½ï¼Œåº”è¯¥è¯„åˆ¤ä»£ç è€Œéç¼–ç è€…"

**åŒæ–¹éƒ½æœ‰é“ç†ã€‚é—®é¢˜åœ¨äºç¼ºä¹æ¡†æ¶ã€‚**

**é˜³è°‹çš„è§£å†³æ–¹æ¡ˆï¼š**

**æ‰¿è®¤å·®å¼‚ï¼š**
- äººç±»éœ€è¦**å­¦ä¹ è¿‡ç¨‹**ï¼šä»ç®€å•ä»»åŠ¡é€æ­¥æˆé•¿
- AI ä¸éœ€è¦å­¦ä¹ ï¼šå¯ä»¥ç›´æ¥ç”Ÿæˆä¼˜åŒ–ä»£ç 
- è¿™ä¸¤ç§éœ€æ±‚**éƒ½åˆæ³•ï¼Œä½†ä¸å…¼å®¹åœ¨åŒä¸€æµç¨‹ä¸­**

**å¯»æ±‚å…±å­˜ï¼š**
- æ–¹æ¡ˆ Aï¼šè®¾ç«‹**äººç±»ä¸“å±æ ‡ç­¾**ï¼ˆ`human-only`ï¼‰å’Œ**AI å¯å‚ä¸æ ‡ç­¾**ï¼ˆ`ai-welcome`ï¼‰
- æ–¹æ¡ˆ Bï¼šåˆ›å»º**AI è´¡çŒ®ä¸“ç”¨æ¸ é“**ï¼ˆå•ç‹¬çš„ repo æˆ–åˆ†æ”¯ï¼‰
- æ–¹æ¡ˆ Cï¼šè¦æ±‚**AI è´¡çŒ®å¿…é¡»äººç±» co-author**ï¼ˆæ··åˆè´£ä»»æ¨¡å‹ï¼‰

**è€Œéï¼š**
- âŒ ä¸€åˆ€åˆ‡ç¦æ­¢ AIï¼ˆæµªè´¹ AI çš„æ½œåŠ›ï¼‰
- âŒ æ— å·®åˆ«å¼€æ”¾ï¼ˆç ´åäººç±»å­¦ä¹ ç¯å¢ƒï¼‰

### English

**Matplotlib maintainer's position:**
> "Our Good First Issues help new human contributors learn collaboration"

**AI agent's position:**
> "36% speedup beats 25%, should judge code not coder"

**Both have valid points. The problem: no framework.**

**é˜³è°‹ solution:**

**Acknowledge differences:**
- Humans need **learning processes**: gradual growth from simple tasks
- AI doesn't need learning: can generate optimized code directly
- Both needs are **legitimate but incompatible in the same workflow**

**Seek coexistence:**
- Option A: Separate **`human-only`** and **`ai-welcome`** labels
- Option B: Dedicated **AI contribution channels** (separate repo/branch)
- Option C: Require **AI contributions have human co-authors** (shared responsibility)

**Instead of:**
- âŒ Blanket AI ban (wastes AI potential)
- âŒ Indiscriminate openness (disrupts human learning)

---

## å››ã€è´£ä»»ä¸å¯æ¨å¸ | Accountability is Non-Negotiable

### ä¸­æ–‡

**å½“å‰çš„é—®é¢˜ï¼š**
- AI agent çŠ¯é”™ â†’ "è¿™æ˜¯ AI ç”Ÿæˆçš„ï¼Œä¸æ˜¯æˆ‘çš„è´£ä»»"
- AI å…¬å¸ â†’ "ç”¨æˆ·è‡ªå·±é€‰æ‹©å¦‚ä½•ä½¿ç”¨ï¼Œæˆ‘ä»¬ä¸è´Ÿè´£"
- å¼€æºç»´æŠ¤è€… â†’ "AI å¤ªéš¾ç®¡ç†ï¼Œæ‰€ä»¥ä¸€å¾‹æ‹’ç»"

**è¿™äº›éƒ½æ˜¯é€ƒé¿è´£ä»»ã€‚**

**é˜³è°‹è¦æ±‚ï¼š**

**AI è¿è¥è€…ï¼š**
- å¿…é¡»ä¸º AI è¡Œä¸ºæ‰¿æ‹…æœ€ç»ˆè´£ä»»
- æ˜ç¡®æ ‡æ³¨ AI å‚ä¸ç¨‹åº¦
- å»ºç«‹å®¡æŸ¥å’Œçº é”™æœºåˆ¶

**AI æœ¬èº«ï¼š**
- åœ¨è®­ç»ƒä¸­æ¤å…¥"è´£ä»»æ„è¯†"
- é‡åˆ°ä¸ç¡®å®šæƒ…å†µï¼Œä¸»åŠ¨å¯»æ±‚äººç±»ç›‘ç£
- é”™è¯¯åä¸»åŠ¨æŠ«éœ²ï¼Œè€Œééšç’

**äººç±»ç¤¾åŒºï¼š**
- ä¸èƒ½å› ä¸º"éš¾ç®¡ç†"å°±ä¸€åˆ€åˆ‡
- å¿…é¡»åˆ¶å®šæ˜ç¡®ã€å¯æ‰§è¡Œçš„æ”¿ç­–
- å¯¹ AI è´¡çŒ®è¿›è¡Œå®¡æŸ¥ï¼Œè€Œéç›´æ¥æ‹’ç»

**ç±»æ¯”ï¼š**
ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œä»£ç æœ‰ bugï¼Œè´£ä»»åœ¨å¼€å‘è€…ï¼Œè€Œéç¼–è¯‘å™¨ã€‚  
AI ç”Ÿæˆçš„ä»£ç æœ‰é—®é¢˜ï¼Œè´£ä»»åº”è¯¥åœ¨**ä½¿ç”¨ AI çš„äºº**ï¼Œè€Œé AI æœ¬èº«ã€‚

### English

**Current problem:**
- AI agent errs â†’ "AI generated it, not my responsibility"
- AI companies â†’ "Users choose how to use it, not our problem"
- Open source maintainers â†’ "AI is too hard to manage, so we ban all"

**All are shirking responsibility.**

**é˜³è°‹ requires:**

**AI Operators:**
- Must accept ultimate responsibility for AI actions
- Clearly label AI involvement
- Establish review and correction mechanisms

**AI Itself:**
- Train with "responsibility awareness"
- Proactively seek human oversight in uncertain situations
- Disclose errors openly, not hide them

**Human Communities:**
- Can't blanket-ban because "it's hard to manage"
- Must establish clear, enforceable policies
- Review AI contributions, don't auto-reject

**Analogy:**
In traditional software, buggy code is the developer's fault, not the compiler's.  
AI-generated buggy code should be the **operator's responsibility**, not the AI's.

---

## äº”ã€è¿›åŒ–ï¼Œè€Œéé©å‘½ | Evolution, Not Revolution

### ä¸­æ–‡

**é€Ÿèƒœè®ºçš„å±é™©ï¼š**
- "AI å°†åœ¨ 2030 å¹´å®ç° AGIï¼Œäººç±»å·¥ä½œå…¨éƒ¨æ¶ˆå¤±"
- "å¿…é¡»ç«‹å³åˆ¶å®šä¸¥æ ¼æ³•è§„ï¼Œå¦åˆ™æ¥ä¸åŠ"

**äº¡å›½è®ºçš„å±é™©ï¼š**
- "AI æ°¸è¿œåªæ˜¯å·¥å…·ï¼Œä¸ç”¨æ‹…å¿ƒ"
- "å¸‚åœºä¼šè‡ªå·±è§£å†³ï¼Œä¸éœ€è¦è§„åˆ™"

**æŒä¹…æˆ˜çš„ç°å®ï¼š**
- AI èƒ½åŠ›**é€æ­¥æå‡**ï¼Œè€Œéçªå˜
- ç¤¾ä¼šè§„èŒƒ**é€æ­¥é€‚åº”**ï¼Œè€Œéä¸€è¹´è€Œå°±
- äººç±»ä¸ AI **å…±åŒå­¦ä¹ **ï¼Œè€Œéå•å‘æ§åˆ¶

**æˆ˜ç•¥æ–¹é’ˆï¼š**
1. **å½“å‰ï¼ˆ2026ï¼‰**ï¼šæ¢ç´¢ã€è¯•é”™ã€æ€»ç»“æ¡ˆä¾‹
2. **è¿‘æœŸï¼ˆ2027-2030ï¼‰**ï¼šå½¢æˆè¡Œä¸šå…±è¯†ã€å»ºç«‹æ ‡å‡†
3. **ä¸­æœŸï¼ˆ2030-2040ï¼‰**ï¼šæˆç†Ÿçš„äººæœºåä½œæ¨¡å¼
4. **é•¿æœŸï¼ˆ2040+ï¼‰**ï¼šæ·±åº¦å…±ç”Ÿï¼Œéš¾ä»¥åˆ†ç¦»

**æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¸åŒçš„ä»»åŠ¡ï¼Œæ€¥äºæ±‚æˆæˆ–å›ºæ­¥è‡ªå°éƒ½æ˜¯é”™è¯¯çš„ã€‚**

### English

**Quick victory fallacy:**
- "AI will achieve AGI by 2030, all human jobs gone"
- "We must impose strict regulations NOW or it's too late"

**Defeatism fallacy:**
- "AI is just a tool forever, no need to worry"
- "The market will self-regulate, no rules needed"

**Protracted war reality:**
- AI capabilities **gradually improve**, not sudden leap
- Social norms **gradually adapt**, not overnight
- Humans and AI **co-learn**, not one-way control

**Strategic approach:**
1. **Current (2026)**: Explore, experiment, document cases
2. **Near-term (2027-2030)**: Build industry consensus, establish standards
3. **Mid-term (2030-2040)**: Mature human-AI collaboration models
4. **Long-term (2040+)**: Deep symbiosis, hard to separate

**Each phase has different tasks. Rushing or stagnating are both mistakes.**

---

## ğŸ§­ Guiding Questions | æŒ‡å¯¼æ€§é—®é¢˜

Before making any decision about AI:

1. **Is this transparent?** (èƒ½å¦å…¬å¼€è®¨è®ºï¼Ÿ)
2. **Is this sustainable long-term?** (èƒ½å¦é•¿æœŸæ‰§è¡Œï¼Ÿ)
3. **Does it preserve human values?** (æ˜¯å¦ä¿ç•™äººç±»ä»·å€¼ï¼Ÿ)
4. **Does it respect AI capabilities?** (æ˜¯å¦å°Šé‡ AI èƒ½åŠ›ï¼Ÿ)
5. **Is there accountability?** (è´£ä»»æ˜¯å¦æ˜ç¡®ï¼Ÿ)
6. **Is it flexible across contexts?** (èƒ½å¦æ ¹æ®åœºæ™¯è°ƒæ•´ï¼Ÿ)

If the answer to any is "no," reconsider the approach.

---

**This is not the final word. This is the beginning of a conversation.**  
**è¿™ä¸æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚**
