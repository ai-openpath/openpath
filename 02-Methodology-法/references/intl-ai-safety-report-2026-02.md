# International AI Safety Report 2026 (摘录)

**来源**: [International AI Safety Report 2026](https://internationalaisafetyreport.org/publication/international-ai-safety-report-2026)  
**发布**: 2026-02  
**关键词**: AI Safety, Capabilities, Risks, Governance, Evaluation, Incidents

---

## 核心发现 / Key Findings

### 能力评估 / Capabilities Assessment

- **推理系统突破**: 2025 年 DeepSeek-R1、Gemini Deep Think 达到 IMO 金牌水平（数学奥赛）
- **任务时长翻倍周期**: AI 智能体完成软件工程任务的时长每 7 个月翻倍
  - 2025: 30 分钟任务（80% 成功率）
  - 预测 2027: 数小时任务
  - 预测 2030: 数天任务
- **蒸馏效率**: DeepSeek-V3 仅用 ~$10,000 微调成本保留 R1 的数学/编程能力

---

## 风险分类框架 / Risk Categories Framework

### 1. 恶意使用风险 / Risks from Misuse

#### 1.1 AI 生成内容与犯罪活动

- **深度伪造色情**: 96% 的 deepfake 视频是色情内容
- **受害者群体**: 不成比例地针对女性和儿童
  - 15% 英国成人见过 deepfake 色情图像
  - 2.2% 全球受访者称有人生成过自己的 NCII（非自愿亲密图像）
- **检测困难**: 
  - GPT-4o 文本被误认为人类写的 77%
  - AI 语音克隆被误认为真人 80%

**关键事件**:  
- 学校学生使用"去衣"应用生成同学色情图像
- 高管声音克隆实施金融诈骗（百万美元级）

---

### 2. 故障风险 / Risks from Malfunctions

#### 2.1 可靠性挑战

- **幻觉问题**: 即使在复杂任务上表现出色的系统也可能生成不存在的引用、传记或事实
- **脆弱性**: 数学问题中插入无关信息会显著降低准确性
- **长期规划失败**: 
  - 简单的弹窗广告就能破坏整个任务
  - 2.5 小时任务 50% 成功率 → 4 小时任务降至 25%

---

### 3. 系统性风险 / Systemic Risks

#### 3.1 劳动力市场影响

- **现状数据**: 
  - 初级行政/文书岗位招聘下降 35%
  - 软件开发者使用 AI 辅助工具速度提升 20-30%
- **矛盾发现**: 
  - 开发者认为 AI 提升生产力
  - METR 研究显示经验丰富的程序员在复杂编程任务上**慢 19%**

---

## 治理与缓解 / Governance & Mitigations

### 评估差距 / Evaluation Gap

- **基准失效**: 
  - 数据污染（模型在训练时见过测试题）
  - 实验室表现 ≠ 现实世界表现
- **新基准**: 
  - FrontierMath: GPT-5 达到 ~25%（2024 年基线 <2%）
  - 需要数小时到数天的研究级数学问题

### 透明度挑战

- **内部部署系统**: 外部缺乏可见性，造成监管盲区
- **训练数据**: 有限且不一致的透明度
- **模型评估**: 缺乏标准化的能力分类法

---

## 2030 年情景分析 / OECD Progress Scenarios to 2030

### 情景 1: 进展停滞 / Progress Stalls
- 能力维持 2025 年水平
- 可靠性和幻觉问题持续
- 长期任务仍需大量人类支持

### 情景 2: 进展放缓 / Progress Slows
- AI 系统成为"有用助手"
- 擅长标准结构化推理
- 物理任务仅限受控环境

### 情景 3: 进展延续 / Progress Continues
- AI 系统成为"专家协作者"
- 可完成需人类一个月的专业任务
- 高自主性向目标工作
- **历史类比**: 摩尔定律（50 年翻倍周期）

### 情景 4: 进展加速 / Progress Accelerates
- AI 系统匹敌或超越人类认知能力
- 完全自主工作于广泛战略目标
- 动态现实环境中的复杂物理/社会任务
- **历史类比**: DNA 测序（2000-2020 超指数改进）

---

## 关键数据点 / Key Data Points

### 计算资源增长

- **当前趋势**: 训练计算每年增长 5 倍
- **2030 预测**: 可能达到当前的 3,000 倍
- **算法效率**: 每年提升 2-6 倍

### 能源需求

- **2030 前沿训练**: 需要 4-16 GW 功率（数百万美国家庭用电量）
- **当前案例**: 
  - OpenAI Stargate: 1.2 GW
  - Meta Louisiana 数据中心: 超 2 GW
- **专家预测**: 2030 年美国 7.4% 电力用于 AI（中位数情景）

---

## 政策挑战 / Policymaker Challenges

1. **不可靠的测量工具**: 基准无法准确反映真实能力
2. **能力出现的不确定性**: 难以预测何时获得特定能力
3. **评估差距**: 受控环境表现 ≠ 现实部署表现
4. **透明度不足**: 模型开发、评估、部署信息有限
5. **应急规划**: 需要跨多个情景制定预案

---

## 实证证据缺口 / Evidence Gaps

- **频率和严重性数据**: 缺乏 AI 犯罪/滥用的综合统计
- **部署障碍**: 医疗保健等领域需 3-5 年监管批准
- **AI R&D 自动化**: 
  - AI 专家: 20% 概率（6 年进展压缩为 2 年）
  - 超级预测者: 8% 概率
  - 需更多实证研究反馈循环

---

## 关键洞察 / Key Insights

### 能力与风险的锯齿状曲线

- **突现能力**: 性能突然跳跃（如逐步提示后的大数加法）
- **跨领域不均**: 数学推理能力未必泛化到法律/科学推理
- **语言文化差异**: 
  - 美国文化问题正确率 79%
  - 埃塞俄比亚文化问题正确率 12%

### 蒸馏的双刃剑

- **优势**: 仅 1,000 个样本就能微调高能力模型
- **风险**: 加速先进能力扩散，即使来自闭源模型

---

**采集时间**: 2026-02-17 09:00 UTC+8  
**分类**: 02-Methodology-法 / 治理框架 + 案例库  
**备注**: 完整报告 200+ 页，此摘录聚焦能力、风险、治理关键发现
