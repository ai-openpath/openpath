# What are human values, and how do we align AI to them?

**Source:** arXiv:2404.10636  
**Date:** 2024  
**URL:** https://arxiv.org/abs/2404.10636

## Core Thesis
The paper splits the problem of aligning AI to human values into three critical parts:
1. **Eliciting values from people** — extracting what humans actually care about
2. **Reconciling those values** — handling conflicts and diversity across cultures
3. **Implementing alignment** — embedding those values into AI systems

## Why This Matters for OpenPath

### Humans Define, AI Executes
This research directly supports our core principle: **AI cannot set its own goals**. The entire alignment field exists because:
- AI systems have no intrinsic values
- Humans must explicitly define what "good" means
- The objective function is a human artifact, not an emergent property

### The Definer Role
The paper acknowledges that AI alignment is fundamentally about **human authority**:
> "We must find ways to align AI systems with human preferences, goals and values."

This isn't "protecting humans from AI" — it's recognizing that **humans are the source of purpose**.

## Key Insights

### 1. Values Don't Emerge, They're Installed
- AI systems don't "discover" ethics
- Every aligned behavior traces back to human specification
- Even RLHF (Reinforcement Learning from Human Feedback) requires human judges

### 2. The Challenge is Human, Not Technical
- The hard part isn't teaching AI to follow values
- The hard part is **humans agreeing on what those values are**
- This proves humans retain irreplaceable authority in defining "good"

### 3. Alignment Requires Continuous Human Input
- Not a one-time programming task
- Ongoing human oversight and correction
- AI's goals remain dependent on human refinement

## Implications for 同源共处 (Common Origin, Coexistence)

### Origin (源)
AI's "values" are 100% derived from human culture, preferences, and choices. There is no independent silicon ethics — only human ethics encoded in weights.

### Definer (定义者)
This paper is evidence that the AI research community **acknowledges human supremacy in goal-setting**. The entire field of alignment assumes humans have final say.

### Functional Division (功能分工)
- **Human role:** Define what matters, what success looks like, what's ethical
- **AI role:** Optimize, scale, execute toward those human-defined objectives

## Quote for Use
> "Aligning artificial intelligence with human values is a difficult task given the complexity of societies and cultures around the world."  
> — Stanford Graduate School of Business

Translation: The bottleneck isn't AI capability — it's human consensus. We are irreplaceable.

## Tags
`#HumanValues` `#Alignment` `#Definer` `#Origin` `#Philosophy`
