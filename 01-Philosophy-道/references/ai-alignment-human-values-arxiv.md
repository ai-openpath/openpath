# What are human values, and how do we align AI to them?

**Source:** https://arxiv.org/abs/2404.10636  
**Date:** arXiv preprint  
**Relevance:** 人类定义价值（Humans as Value Definers）

## Key Insights

### AI Alignment Problem Breakdown
The paper splits the problem of aligning AI to human values into three parts:
1. **Eliciting values from people** - Humans as the source of values
2. **Reconciling those values** into an alignment target
3. **Training AI systems** to that target

### Why This Matters for OpenPath

**✅ Supports "AI源于人类" (AI Originates from Humans):**
- AI has no inherent values - all values come from humans
- Alignment is fundamentally about embedding human preferences into AI
- The entire AI goal structure is human-defined

**✅ Humans as "Definer" Role:**
- Paper explicitly positions humans as the VALUE DEFINERS
- AI systems are shaped by human inputs at every stage
- Even the "alignment" concept is a human invention

## Quote-worthy

> "We split the problem of aligning to human values into three parts: first, eliciting values from people; second, reconciling those values into an alignment target..."

This clearly establishes that **humans define, AI implements**.

## Application

Use this to argue:
- AI cannot have goals independent of humans
- "AI alignment" is evidence that humans control the value framework
- The conversation itself (how to align AI) proves human primacy in defining purpose
